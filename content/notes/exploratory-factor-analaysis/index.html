---
title: "Exploratory Factor Analysis (EFA)"
authors:
- admin
date: 2022-01-30T21:13:14-05:00
output:
  html_document:
    highlight: textmate
categories: ["R"]
subtitle: Guide for doing an exploratory factor analysis (EFA) in R.
summary: This is a guide for doing an exploratory factor analysis using R. The guide is intended as preparatory in doing confirmatory factor analysis.
tags: ["EFA", "Exploratory", "Factor analysis"]
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<style type="text/css">
.scroll-200 {
  max-height: 200px;
  overflow-y: auto;
  background-color: inherit;
}
</style>
<div id="table-of-contents" class="section level1">
<h1>Table of contents</h1>
<p><a href="#objectives">1. Objectives of factor analysis</a></p>
<p><a href="#design">2. Designing an EFA</a></p>
<p><a href="#assumption">3. Assumption in Exploratory factor analysis</a></p>
<p><a href="#deriving">4. Deriving factors</a></p>
<p><a href="#assessment">5. Assessing overall fit</a></p>
<p><a href="#interpretation">6. Interpreting factors</a></p>
<hr />
<p><br></p>
</div>
<div id="objectives-of-factor-analysis" class="section level1">
<h1>1. Objectives of factor analysis <a name="objectives"></a></h1>
<p><img src="materials/overview_factor.jpg" width="50%" style="display: block; margin: auto;" /></p>
<p>This EFA guide aims primarily on preparing researcher in doing confirmatory factor analysis (CFA) and the structural equation modelling (SEM). To better understand the relation of EFA with other factor analysis, consider the figure above. The basic assumption of factor analysis is that for a collection of observed variables there are set of underlying variables called factors that can explain the interrelationship among variables.</p>
<p>The primary purpose of EFA is to define the underlying structure among the variables. Researchers consider the method as an exploratory approach useful in searching for structure among a set of variable or as a data reduction method.</p>
<p>Basically the general purpose is to find a way to condense or summarize the information contained in a number of original variables into a smaller set of new, composite dimensions or factors with a minimum loss of information.</p>
<p>EFA focuses on four issues:</p>
<p><strong>1. Specifying the unit analysis</strong></p>
<p>A researcher should first decide the unit of analysis, is it variable or cases. If the objective is to summarize characteristics or variables, a factor analysis is appropriate. A factor analysis is applied to a correlation matrix of the variables to identify the dimensions for the variables that are latent.</p>
<p>If the objective is to summarize cases (e.g., respondents), a cluster analysis for example would be the possible method. It is a factor analysis applied to a correlation matrix of the individual cases based on their characteristics.</p>
<p><strong>2. Data summarization or reduction</strong></p>
<p>In EFA, all variables are simultaneously considered without distinction as to dependent or independent variables. Most researchers use data summarization to group of variables and then do data reduction to identify representative variables from those grouped variables.</p>
<p><strong>Summarization</strong></p>
<ul>
<li><p>Individual variables are grouped and then viewed not what they represent individually, but for what they represent collectively in expressing a concept.</p></li>
<li><p>The goal is to define a small number of factors that adequately represent the original set of variables.</p></li>
</ul>
<p><strong>Reduction</strong></p>
<ul>
<li>The main purpose is to retain the nature and character of the original variables but reduce the number of actual values included in the analysis.</li>
</ul>
<p><strong>3. Variable selection</strong></p>
<p>In both uses - summarization and reduction, the researcher should specify the potential dimension that can be identified through the character and nature of variable subjecting for factor analysis.</p>
<p>Researcher should use judgement on which set of variables to be analyze together. Since factor could always be produced, this can be potential for “garbage in, garbage out”. If researcher indiscriminately includes larger number and just rely factor analysis will figure it out, the possibility of poor results is high. The quality of and meaning of the derived factors should reflect the conceptual underpinnings.</p>
<p><strong>4. Using factor analysis with other multivariate techniques</strong></p>
<p>A high correlation among independent variable for example, are excellent reason for performing data reduction. A variable may have little incremental predictive power if the variable is highly correlated with other explanatory variable in the model. Although it does not mean that the other variable of the factor are less important or have less impact, but instead their effect is already represented by the included variable from the factor.</p>
<p><br></p>
<hr />
<p><br></p>
</div>
<div id="designing-an-efa" class="section level1">
<h1>2. Designing an EFA <a name="design"></a></h1>
<p>According to Hair et al. (2018), the design of EFA involves the following basic decisions:</p>
<p><strong>1. Variable selections and measurement issues</strong></p>
<p>So, what type of variables can be used in factor analysis?</p>
<ul>
<li><p>Primarily a correlation value can be calculated among all variables.</p></li>
<li><p>Metric variables are more easily measured by several types of correlation.</p></li>
<li><p>Non-metric variables are more challenging. The use of dummy variables such as Boolean factor analysis can be use.</p></li>
</ul>
<p><strong>2. Sample size</strong></p>
<p>Generally researcher would not factor analyze a sample fewer than 50 observations, could be 100 or larger. While other suggested large sample of 2000 and larger as the number of variables and expected number of factor increases.</p>
<p>Another criterion is the ratio of cases to variables, the general rule is to have a minimum of five times as many observations as the number of variables to be analyzed. More acceptable would be a 10:1 ratio, while other proposes 20 cases for each variables.</p>
<p><br></p>
<hr />
<p><br></p>
</div>
<div id="assumption-in-efa" class="section level1">
<h1>3. Assumption in EFA <a name="assumption"></a></h1>
<p>The critical assumptions underlying exploratory factor analysis are more conceptual than statistical. The concerns center as much on the character and composition of variable included in the analysis as on their statistical qualities.</p>
<p><strong>Conceptual issues</strong></p>
<p>The technique has no means of determining appropriateness other than the correlations among variables. It is the responsibility of the researcher to ensure that the observed patterns are conceptually valid and appropriate to study.</p>
<p>The researcher must ensure the sample is homogenous with respect to the underlying factor structure. If there would be groups that are expected to have different items measuring the concept, seperate EFA should be performed.</p>
<p><strong>Statistical issues</strong></p>
<p>Departures from normality, homoscedasticity, and linearity apply only to the extent that they diminish the observed correlation. Some degree of multicollinearity is desirable because objective is to identify interrelated sets of variables.</p>
<div id="overall-measurement-of-intercorrelation" class="section level2">
<h2>Overall measurement of intercorrelation</h2>
<p>The researcher must ensure that the data matrix has sufficient correlation to justify the application of EFA. There are two tests commonly used in assessing factorability, the KMO and Bartlett test.</p>
<p><strong>Bartlett test</strong></p>
<p>The test examines the entire correlation matrix. Test the hypothesis that correlation matrix is an identity matrix. It is a statistical test for the presence of correlation among the variables. Providing statistical significance indicating the correlation matrix has significant correlation among at least some of the variables.</p>
<p>Increasing sample size causes the Bartlett test to become more sensitive in detecting correlation among the variables.</p>
<p>We use the <code>BARTLETT()</code> function from <code>EFAtools</code> to conduct the test. A significant results signifies data are appropriate for factor analysis.</p>
<pre class="r"><code>BARTLETT(x = data,
         N = nrow(data))</code></pre>
<pre><code>i &#39;x&#39; was not a correlation matrix. Correlations are found from entered raw data.</code></pre>
<pre><code>Warning in BARTLETT(x = data, N = nrow(data)): ! &#39;N&#39; was set and data entered. Taking N from data.</code></pre>
<pre><code>
v The Bartlett&#39;s test of sphericity was significant at an alpha level of .05.
  These data are probably suitable for factor analysis.

  &lt;U+0001D712&gt;²(105) = 3342.25, p &lt; .001</code></pre>
<p><strong>Kaiser-Meyen-Olkin (KMO)</strong></p>
<p>An index ranges from 0 to 1, approaching 1 means each variable is perfectly predicted without error by other variables.</p>
<p>It is a statistic indicating the proportion of variance in your variables that might be caused by underlying factor.</p>
<p>Some guidelines:</p>
<ul>
<li><span class="math inline">\(\ge 0.90\)</span> - marvelous</li>
<li><span class="math inline">\(\ge 0.80\)</span> - meritorious</li>
<li><span class="math inline">\(\ge 0.70\)</span> - middling</li>
<li><span class="math inline">\(\ge 0.60\)</span> - mediocre</li>
<li><span class="math inline">\(\ge 0.50\)</span> - miserable</li>
<li><span class="math inline">\(&lt; 0.50\)</span> - unacceptable</li>
</ul>
<p>The MSA values may increase from the following:</p>
<ol style="list-style-type: decimal">
<li>sample size increases</li>
<li>the average correlation increases</li>
<li>the number of variable increases</li>
<li>the number of factor decreases</li>
</ol>
<p>If MSA falls below 0.50, then variable specific MSA values can identify variable for deletion to achieve an overall value of greater than 0.50 overall MSA value. May follow the following steps.</p>
<ol style="list-style-type: decimal">
<li>Identify the variables with lowest MSA subject for deletion then recalculate factor analysis.</li>
<li>Repeat the process until an MSA 0.50 and above is achieved.</li>
</ol>
<p>Purpose of elimination with MSA under 0.50 is that these variables’ correlation with other variables are poorly representing the extracted factors.</p>
<p>Often low MSA end up as a single variable on a factor result from their lack of association with any of the other variables. It means they are not correlated high enough with other variables in the analysis to be suitable for EFA.</p>
<p>In the sample data, both the overall and individual MSA ranging from meritorious to marvelous.</p>
<pre class="r"><code>KMO(data)</code></pre>
<pre><code>i &#39;x&#39; was not a correlation matrix. Correlations are found from entered raw data.</code></pre>
<pre><code>
-- Kaiser-Meyer-Olkin criterion (KMO) ------------------------------------------

v The overall KMO value for your data is marvellous.
  These data are probably suitable for factor analysis.

  Overall: 0.902

  For each variable:
bridging1 bridging2 bridging3  bonding1  bonding2  bonding3  bonding4       ca1 
    0.884     0.876     0.911     0.924     0.919     0.879     0.931     0.924 
      ca2       ca3       ca4       om1       om2       om3       om4 
    0.902     0.902     0.901     0.925     0.925     0.856     0.890 </code></pre>
<p><br></p>
<hr />
<p><br></p>
</div>
</div>
<div id="deriving-factors" class="section level1">
<h1>4. Deriving factors <a name="deriving"></a></h1>
<p>Exploring possible factors refer to identifying the underlying structure of relationship. The decision must be made concerning;</p>
<ol style="list-style-type: decimal">
<li>The method of extracting the factors (CFA vs PCA), and;</li>
<li>The number of factors selected to represent the underlying structure in the data.</li>
</ol>
<p>In this guide, we already have a prior idea of the expected number of factors to be extracted before undertaking the factor analysis. However, we will use a scree test and parallel analysis to check if the number of underlying factors from the tests conforms with our expectation.</p>
<p>Common to the tests demonstrated here follows the famous eigenvalues greater than 1. The rationale for this is that any individual factor should account for the variance of at least a single variable if it is to be retained for interpretation.</p>
<p>Note that an eigenvalue is simply the sum of the squared loading (variance) of variables on that factor.</p>
<p>Rule is, do not retain any factors which account for less variance than a single variable.</p>
<p><strong>Scree test</strong></p>
<p>The scree test is used to identify the optimum number of factors that can be extracted before the amount of unique variance begins to dominate the common variance structure. Refer to the accompanying notes for more details on unique and common variance.</p>
<p>All factors contain at least some unique variance, while the proportion of unique varince is substantially higher in later factors.</p>
<p>The plot is derived by plotting the latent roots against the number of factors in their order of extraction.</p>
<p>The inflection point “elbow” - point at which the curve first begins to straighten out- is considered to represent factors containing more unique rather common variances and thus less suitable for retention.</p>
<pre class="r"><code>scree(data)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/scree-test-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p><strong>Test 2: Parallel analysis</strong></p>
<p>The procedure generates a large number of simulated dataset with random values for the same number of variables and sample sizes. Then each of these simulated dataset is then factor analyzed, either with PCA and CFA and the eigenvalues are averaged for each factor across all the dataset. The results is the average eigenvalues for the first, second and so on across the set of simulated dataset. These values are then compared to the eigenvalues extracted for the original data. All factors with eigenvalues above those average eigenvalues are retained.</p>
<p>The parallel analysis is not yet incorporated into the major statistical packages like SPSS.</p>
<pre class="r"><code>fa.parallel(data, fm = &quot;ml&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/parallel-analysis-1.png" width="70%" style="display: block; margin: auto;" /></p>
<pre><code>Parallel analysis suggests that the number of factors =  4  and the number of components =  3 </code></pre>
<p><strong>Kaiser-Guttman Criterion</strong></p>
<p>The criterion is also known as “eigenvalues greater than 1 rule”. Hence, the test suggests how many factors to retain based on eigenvalues greater than 1.</p>
<p>In the code, we predetermined the number of possible factors for exploratory factor analysis. The results recommends 3 factors to retained. The bonding and bridging construct reflect under social capital, the two factors may have been grouped in one resulting only to three recommended factors.</p>
<pre class="r"><code>KGC(x = data,
    eigen_type = c(&quot;PCA&quot;, &quot;EFA&quot;, &quot;SMC&quot;),
    n_factors = 4)</code></pre>
<pre><code>i &#39;x&#39; was not a correlation matrix. Correlations are found from entered raw data.</code></pre>
<pre><code>
Eigenvalues were found using PCA, EFA, and SMC.

-- Number of factors suggested by Kaiser-Guttmann criterion --------------------

( ) With PCA-determined eigenvalues:  3
( ) With SMC-determined eigenvalues:  3
( ) With EFA-determined eigenvalues:  3</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="70%" style="display: block; margin: auto;" /><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-2.png" width="70%" style="display: block; margin: auto;" /><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-3.png" width="70%" style="display: block; margin: auto;" /></p>
<p><br></p>
<hr />
<p><br></p>
</div>
<div id="assessing-overall-fit" class="section level1">
<h1>5. Assessing overall fit <a name="assessment"></a></h1>
<div id="extracting-factor-components" class="section level2">
<h2>Extracting factor components</h2>
<p>The process of estimating factors and loadings involve the selection fo the principal component analysis versus the common factor. Principal component analysis (PCA) is used when the objective is to summarize the most of the original information (variance). It considers the total variance and derivatives factors that contain small proportions of unique variance and, in some instances, error variance. Whereas, common factor analysis is used primarily to identify underlying factors or dimensions that reflect what the variables share in common. It considers only the common or shared variance, assuming that both the unique and error variance are not of interest in defining the structure of the variables.</p>
<p>The most important tool in interpreting the factor is ‘factor rotation’. The ultimate effect of rotating the factor matrix is to redistribute the variance from earlier factors to later ones to achieve a simpler, theoretically more meaningful factor pattern. Factor rotation achieve a simpler and theoretically more meaningful factor solutions. In most cases rotation of the factors improves the interpretation by reducing some of the ambiguities that often accompany initial unrotated factor solutions. Categorically, rotation falls under orthogonal and oblique factor rotation.</p>
</div>
<div id="orthogonal-factor-rotation" class="section level2">
<h2>Orthogonal Factor Rotation</h2>
<p>Orthogonal rotation are more frequently applied because the analytical procedure for performing oblique rotations are not as well developed and are still subject to some controversy.</p>
<p><strong>Varimax</strong></p>
<p>The varimax is the mostly widely used orthogonal rotation method. The criterion centers on simplifying the columns of the factor matrix. It maximizes the sum of variance of required loadings of the factor matrix. Results are some high loadings (i.e., close to -1 or +1) along with some loadings near 0 in each column of the matrix. Closer to -1 or +1 indicating a clear positive or negative associations between variables and factors. Close to 0 indicate a clear lack of association. Varimax rotation often give a clear separation of factors.</p>
<p>Using <code>pscyh</code> package and the previous tests on the factorability of the data, four factors were specified with varimax rotation. We added an argument <code>cor</code> to specify the type of method in extracting the correlation coefficient or factor loadings. Since the data are in a response scale, pearson correlation may be less suitable as it requires measurement to be continuous. Here we used a polychoric correlation to compute for the factor loading. The use of polychoric correlation permit the use of any combination of categorical and continuous indicator.</p>
<pre class="r"><code>efa_varimax &lt;- psych::fa(r = data,
                         nfactors = 4, 
                         rotate = &quot;varimax&quot;,
                         cor = &quot;poly&quot;)

# printing the results, excluding loadings less than 0.6
print(efa_varimax$loadings, cutoff = 0.6)</code></pre>
<pre><code>
Loadings:
          MR2   MR1   MR3   MR4  
bridging1                   0.788
bridging2                   0.837
bridging3                   0.742
bonding1        0.716            
bonding2        0.774            
bonding3        0.854            
bonding4        0.706            
ca1                   0.742      
ca2                   0.758      
ca3                   0.763      
ca4                   0.749      
om1       0.759                  
om2       0.773                  
om3       0.869                  
om4       0.790                  

                 MR2   MR1   MR3   MR4
SS loadings    3.062 2.976 2.811 2.449
Proportion Var 0.204 0.198 0.187 0.163
Cumulative Var 0.204 0.403 0.590 0.753</code></pre>
<p><strong>Quartimax</strong></p>
<p>Centers on simplifying rows of a factor matrix. Focuses on rotating the initial factors so that a variable loads high on one factor and as low as possible on all other factors. Many variables can load high or near high on the same factors because the technique centers on the simplifying the rows. Quartimax often does not proved successful in producing simpler structures.</p>
<p>The quartimax identified bridging and bonding items under one construct. Although the results is higly likely since both bonding and bridging refers to social capital concept.</p>
<pre class="r"><code>efa_quartimax &lt;- fa(r = data,
                    nfactors = 4,
                    rotate = &quot;quartimax&quot;,
                    cor = &quot;poly&quot;)</code></pre>
<pre><code>Loading required namespace: GPArotation</code></pre>
<pre class="r"><code>print(efa_quartimax$loadings, cutoff = 0.6)</code></pre>
<pre><code>
Loadings:
          MR1    MR2    MR3    MR4   
bridging1  0.805                     
bridging2  0.840                     
bridging3  0.784                     
bonding1   0.837                     
bonding2   0.833                     
bonding3   0.826                     
bonding4   0.756                     
ca1                      0.682       
ca2                      0.700       
ca3                      0.700       
ca4                      0.698       
om1               0.760              
om2               0.774              
om3               0.874              
om4               0.796              

                 MR1   MR2   MR3   MR4
SS loadings    5.384 3.112 2.105 0.698
Proportion Var 0.359 0.207 0.140 0.047
Cumulative Var 0.359 0.566 0.707 0.753</code></pre>
<p><strong>Equamax</strong></p>
<p>A compromise between varimax and quartimax. Rather concentrating on simplification of the rows or on simplification of the column, it tries to accomplish some of each. The equamax has bot gained widespread acceptance and is used infrequently.</p>
<pre class="r"><code>efa_equamax &lt;- fa(r = data,
                    nfactors = 4,
                    rotate = &quot;equamax&quot;,
                    cor = &quot;poly&quot;)

print(efa_equamax$loadings, cutoff = 0.6)</code></pre>
<pre><code>
Loadings:
          MR2   MR1   MR4   MR3  
bridging1             0.814      
bridging2             0.862      
bridging3             0.767      
bonding1        0.695            
bonding2        0.756            
bonding3        0.842            
bonding4        0.690            
ca1                         0.736
ca2                         0.752
ca3                         0.758
ca4                         0.745
om1       0.756                  
om2       0.771                  
om3       0.867                  
om4       0.787                  

                 MR2   MR1   MR4   MR3
SS loadings    3.018 2.788 2.767 2.726
Proportion Var 0.201 0.186 0.184 0.182
Cumulative Var 0.201 0.387 0.572 0.753</code></pre>
<p><strong>Oblique rotation</strong></p>
<p>More flexible as this rotation need not to be orthogonal. Allow correlated factors instead of maintaining independence between the rotated factors. More realistic because theoretically important underlying dimension are not assumed to be uncorrelated with each other. Since in reality more or less variables has some degree of association. The promax and oblimin are the widely used rotation under oblique rotation. When statistical softwares were not used, promax provide easier manual computation than oblimin, however the used of computer eliminate this advantage of promax over oblimin. Today, oblimin are used more than promax although both provide quite the same output.</p>
<p>In comparison with the sample data, oblimin provide better extraction based on the factor loadings.</p>
<div id="promax" class="section level3">
<h3>Promax</h3>
<pre class="r"><code>efa_promax &lt;- fa(r = data,
                    nfactors = 4,
                    rotate = &quot;promax&quot;,
                    cor = &quot;poly&quot;)

print(efa_promax$loadings, cutoff = 0.6)</code></pre>
<pre><code>
Loadings:
          MR2    MR1    MR3    MR4   
bridging1                       0.870
bridging2                       0.946
bridging3                       0.810
bonding1          0.714              
bonding2          0.840              
bonding3          1.008              
bonding4          0.769              
ca1                      0.807       
ca2                      0.843       
ca3                      0.808       
ca4                      0.835       
om1        0.819                     
om2        0.844                     
om3        0.933                     
om4        0.819                     

                 MR2   MR1   MR3   MR4
SS loadings    2.956 2.842 2.723 2.384
Proportion Var 0.197 0.189 0.182 0.159
Cumulative Var 0.197 0.387 0.568 0.727</code></pre>
<p><strong>Oblimin</strong></p>
<pre class="r"><code>efa_oblimin &lt;- fa(r = data,
                    nfactors = 4,
                    rotate = &quot;oblimin&quot;,
                    cor = &quot;poly&quot;)

print(efa_oblimin$loadings, cutoff = 0.6)</code></pre>
<pre><code>
Loadings:
          MR2    MR3    MR1    MR4   
bridging1                       0.865
bridging2                       0.939
bridging3                       0.805
bonding1                 0.697       
bonding2                 0.820       
bonding3                 0.985       
bonding4                 0.751       
ca1               0.806              
ca2               0.843              
ca3               0.806              
ca4               0.833              
om1        0.816                     
om2        0.841                     
om3        0.929                     
om4        0.816                     

                 MR2   MR3   MR1   MR4
SS loadings    2.934 2.714 2.709 2.355
Proportion Var 0.196 0.181 0.181 0.157
Cumulative Var 0.196 0.377 0.557 0.714</code></pre>
<p><br></p>
<hr />
<p><br></p>
</div>
</div>
</div>
<div id="interpreting-factors" class="section level1">
<h1>6. Interpreting factors <a name="interpretation"></a></h1>
</div>
